Got it. I’ll research examples of read-only, knowledge-based MCP servers used by Sourcegraph, GitHub Copilot Enterprise, DeepWiki, and others—especially those designed for source code search, developer onboarding, or internal code documentation queries. I’ll focus on how they describe and structure their MCP tool definitions to ensure the host LLM triggers them correctly.

I’ll also include relevant patterns and recommendations for crafting your Teamcenter MCP server tool descriptions to ensure high invocation accuracy by agent-mode LLMs across various IDEs.

# Knowledge-Retrieval MCP Servers for Developer Teams

## Overview: Read-Only Code & Doc Retrieval Tools

In enterprise developer environments, **MCP (Model Context Protocol) servers** provide LLM assistants with read-only access to codebases and documentation. These tools act as **knowledge retrieval backends** – they let AI “see” source code, internal docs, or knowledge bases without modifying anything. Examples include **Sourcegraph Cody’s code search**, **GitHub’s Copilot knowledge bases**, and **DeepWiki’s repo Q\&A**. By standardizing external context access, MCP servers function like a *“USB-C port” for code intelligence*, ensuring an AI coding assistant can fetch relevant info on demand instead of hallucinating or guessing.

## How MCP Servers Describe Their Tools (Auto-Discovery)

Each MCP server exposes one or more **tools** (functions) described by a name, natural-language description, and JSON schema for inputs/outputs. These descriptions are crucial: they signal to the LLM when to invoke the tool. In practice, an MCP client will either *auto-discover* available tools or require the assistant to list them. For example, when OpenAI’s API connected to the DeepWiki server, the first response was an `mcp_list_tools` payload enumerating the tools with their descriptions. Clients like Cursor or Claude Desktop automatically load the MCP tool list, so the LLM “knows” what actions it can perform. **Auto-discovery** means if a user asks a code-related question, the agent will scan tool descriptions for relevance and trigger the appropriate one. As Cursor’s docs note, “the Agent will automatically use any MCP tools... if it determines them to be relevant”.

**Best practice:** use clear, specific wording in the tool **name** and **description** that mirrors developer queries. The LLM’s decision to use a tool often hinges on keyword overlap or semantic match between the user’s request and the tool description. For instance, a tool described as “**Search code in Sourcegraph**” is likely to be invoked when the user asks *“Find where X function is used”*. Likewise, a tool called “**Ask any question about a GitHub repository**” will be chosen if the user poses a natural language question about a repo. In short, **describe tools exactly in the way you want the model to think of their purpose**.

## Concrete Tool Description Examples

Real-world knowledge-retrieval MCP servers illustrate these patterns. Below are examples of tool definitions (name, description, and input schema) from such servers:

* **Sourcegraph Code Search MCP** – *Tool:* `search` – **Description:** “Search code in Sourcegraph with a given query and limiting results”. *Inputs:* likely a query string and optional limit. *(Another tool `getFileContent` is described as “Retrieve specific file contents from a repository”.)* These straightforward descriptions contain *“search code”* and *“file contents”*, cueing the AI to call them for code search or file retrieval tasks.

* **GitHub Copilot’s MCP Server** – *Tool:* `searchCode` – **Description:** “Search for code within repositories”. *Inputs:* search keywords plus repository context. GitHub’s official MCP toolset also includes `getFileContents`, `listIssues`, etc., each with self-explanatory names (e.g. `listIssues` – “List issues for a repository”). These names align with GitHub API concepts, ensuring the model maps user requests (“show open issues”, “find code matching X”) to the correct action. The **JSON schema** defines required parameters like repository identifier or search query, which the LLM fills in from the user’s prompt.

* **DeepWiki Repository Q\&A** – *Tool:* `ask_question` – **Description:** “Ask any question about a GitHub repository.” Inputs are defined in JSON schema as: a `repoName` (repository identifier) and a `question` string. For example, the schema requires: `"repoName": "owner/repo"` and `"question": "The question to ask about the repository"`. This allows an LLM to call `ask_question` whenever a user query is repository-specific (*“What does function Y do in repo Z?”*). DeepWiki’s server also provides lower-level tools like `read_wiki_structure` (“Get a list of documentation topics for a GitHub repository”) and `read_wiki_contents` (“View documentation about a GitHub repository”) for browsing the generated docs. Notably, those descriptions include *“documentation”* and *“GitHub repository”* keywords, which directly tie to documentation lookup requests.

Each of these examples shows consistent patterns: the **tool name** is concise and camel-cased, while the **description** is a plain-English sentence starting with a verb (Search, Retrieve, Ask, List, etc.) summarizing its function. The **JSON input schema** further documents parameters (often including type and a short description per field) to guide the model’s usage. This explicit schema helps the AI supply correct arguments (e.g. knowing to provide a `repoName` like “facebook/react”).

## Designing Descriptions for Reliable Triggering

To maximize the chances your knowledge-centric MCP server is invoked when needed, consider these best practices:

* **Include Domain Keywords:** Make sure the description contains the domain terms a developer would use in queries. For code search tools, words like *“search code”, “in repository”* are essential. For documentation Q\&A, phrases like *“question about repository”* or *“documentation topics”* signal relevance. This alignment helps the LLM’s internal heuristics match user intents to the tool.

* **Keep Descriptions Focused:** Describe *what* the tool does and *for what context* in one sentence. Avoid ambiguity or overly broad language. For example, **bad:** “Fetch data from system” vs. **good:** “Get the contents of a file from a repository” – the latter is explicit and narrow, which reduces the chance of the AI misusing the tool.

* **Emphasize Read-Only Nature (if important):** If the MCP server is strictly a read-only knowledge base, you can hint at that in the descriptions or tool naming. For instance, naming a tool `searchDocs` or using wording like “retrieve” or “view” signals no side effects. (In contrast, a write-capable tool might use verbs like “create” or “update” – e.g. `createIssue` – which your server might intentionally omit in a read-only context.) Keeping all tools read-only is a safety design when the server is meant as a **“search fallback”** or *expert consultant* rather than an action executor.

* **Provide Examples (Out-of-Band):** While not part of the JSON schema per se, consider documenting a few natural language examples of how to invoke the tool (some servers include these in docs or comments). For instance, Auth0’s MCP reference lists queries like *“Show me all my Auth0 applications.”* next to the `auth0_list_applications` tool. This can train or inform the LLM on expected usage. In a dev assistant context, example prompts like *“Find uses of `fooBar` in our codebase”* or *“Open the README for Repo X”* could be associated with your tools during fine-tuning or system instructions.

* **Use Self-Descriptive Parameter Names:** The JSON parameters should be intuitively named (e.g. `query`, `repo`, `filePath`). The model might mention them when formulating the function call. DeepWiki’s use of `repoName` and `question` is a good pattern; similarly Sourcegraph’s tool likely would use `query` or `searchText`. Clear parameter naming prevents confusion in multi-step interactions.

By following these practices, you ensure the LLM “sees” your MCP server as an **expert knowledge source** and knows exactly when to call it. The aim is to have the assistant defer to the MCP tool whenever a user’s question requires precise information from code or docs – essentially making the server a *trusted oracle for code intelligence*. As an example, GitHub’s own MCP integration allows Copilot to fetch real repo info: when a developer asks something in Copilot Chat with a linked knowledge base, *“GitHub Copilot will search the knowledge base for relevant information and synthesize a response.”*. In the same way, a well-described MCP server will be reliably triggered as the AI’s first step in answering technical queries, rather than the model relying on its training data alone.

## Acting as an “Expert Consultant” in the IDE

Finally, consider some design recommendations so your MCP server feels like an **expert consultant or smart coding companion** in practice:

* **Granular Tools, Unified Purpose:** It often works well to offer a couple of focused tools rather than one monolithic function. DeepWiki’s trio (`read_wiki_structure`, `read_wiki_contents`, `ask_question`) is a great example. They cover browsing, reading, and querying – giving the AI flexibility to either fetch context or answer directly. Together, they make the assistant act like a knowledgeable teammate who can both navigate documentation and answer questions.

* **Use Hierarchical Invocations:** Some servers internally decide which resource to use based on the query. For instance, Sourcegraph’s Cody agent can use MCP to pull in either database schemas or issue trackers or code search results depending on the request. Your tool descriptions can hint at this capability (e.g. a single `searchKnowledgeBase` tool might search multiple sources under the hood). However, be cautious – if the description is too broad, the model might overuse it. An alternative is exposing separate tools for separate data types (like `searchCode` vs. `searchDocs`), so the AI picks the one that best matches the query.

* **Fail Gracefully and Informatively:** As a read-only consultant, the MCP server should handle unfound info gracefully – e.g. return a polite “no results” message or an empty result that the AI can interpret. This way, if the user asks about something outside the codebase, the assistant can fall back to apologizing or general reasoning, rather than getting confused. Designing the output schema or responses to include a confidence or result count can help the AI decide how to incorporate the information (or lack thereof) into its final answer.

* **Integration & Security:** Ensure your MCP server is easily integrated into IDEs (VS Code, Cursor, JetBrains, etc.) with minimal setup. Many platforms support one-click OAuth installations for popular servers. Following that pattern (providing a manifest or URL that the IDE can auto-configure) will encourage usage. Also, clearly document the scopes or read permissions needed (as in the Sourcegraph example which requires an API token with certain scopes). This builds trust that your server is enterprise-ready.

In summary, a **knowledge-retrieval-focused MCP server** should be described in a way that **LLMs instinctively recognize its utility for code-related queries**. By using precise naming, descriptive language tied to developer intents, and robust JSON schemas, you make it *easy* for the AI to call your tool at the right time. Real products like Sourcegraph’s Cody and GitHub’s Copilot Chat have shown the value of this approach – developers can ask natural questions and the AI seamlessly fetches answers from the company’s own code and docs. With careful tool design, your MCP server becomes a smart “team mentor,” always on hand to provide source-level intelligence as an expert consultant or search fallback for your dev team.

**Sources:** Connected examples and documentation for Sourcegraph Cody, GitHub Copilot, DeepWiki, and MCP best practices were used to compile these recommendations. Key references include Sourcegraph’s blog announcement of MCP support, GitHub’s MCP server docs, the DeepWiki MCP tool definitions, and general MCP integration guides, among others detailed above.
